#!/bin/tcsh

#SBATCH --job-name="train_neural_nets"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="batch"
#SBATCH --nodes=1
#SBATCH --ntasks=8           # 8 tasks per node
#SBATCH --cpus-per-task=2
#SBATCH --ntasks-per-node=8  # 8 GPUs per node
#SBATCH --exclusive
#SBATCH --time=30:00:00
#SBATCH --array=1-16
#SBATCH --exclude=h28n12
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=train_neural_nets_%A_%a.out

module load cuda/10.1
source /scratch2/BMC/gsd-hpcs/Jebb.Q.Stewart/conda3.7/etc/profile.d/conda.csh
conda activate base

set CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_standalone/ml4convection"
set TOP_TEMPLATE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_models/lf_experiment05/templates"
set TOP_OUTPUT_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_models/lf_experiment05"

set BATCH_SIZE=60

set TRAINING_PREDICTOR_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_project/predictors/quality_controlled/partial_grids"
set VALIDN_PREDICTOR_DIR_NAME="${TRAINING_PREDICTOR_DIR_NAME}"
set TRAINING_TARGET_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_project/targets/new_echo_classification/no_tracking/omit_north_radar/partial_grids"
set VALIDN_TARGET_DIR_NAME="${TRAINING_TARGET_DIR_NAME}"

set FOURIER_TRANSFORM_FLAGS=(1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0)
set WAVELET_TRANSFORM_FLAGS=(0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1)
set BASE_LOSS_FUNCTION_NAMES=("xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy" "xentropy")
set MIN_TARGET_RESOLUTIONS_DEG=(0.0000 0.0000 0.0000 0.0000 0.0500 0.1000 0.2000 0.4000 0.0000 0.0000 0.0000 0.0000 0.0500 0.1000 0.2000 0.4000)
set MAX_TARGET_RESOLUTIONS_DEG=(0.0500 0.1000 0.2000 0.4000 10000000000.0000 10000000000.0000 10000000000.0000 10000000000.0000 0.0500 0.1000 0.2000 0.4000 10000000000.0000 10000000000.0000 10000000000.0000 10000000000.0000)

set fourier_transform_targets=${FOURIER_TRANSFORM_FLAGS[$SLURM_ARRAY_TASK_ID]}
set wavelet_transform_targets=${WAVELET_TRANSFORM_FLAGS[$SLURM_ARRAY_TASK_ID]}
set base_loss_function_name=${BASE_LOSS_FUNCTION_NAMES[$SLURM_ARRAY_TASK_ID]}
set min_target_resolution_deg=${MIN_TARGET_RESOLUTIONS_DEG[$SLURM_ARRAY_TASK_ID]}
set max_target_resolution_deg=${MAX_TARGET_RESOLUTIONS_DEG[$SLURM_ARRAY_TASK_ID]}

set min_target_resolution_deg=`printf "%.4f" $min_target_resolution_deg`
set max_target_resolution_deg=`printf "%.4f" $max_target_resolution_deg`

set template_file_name="${TOP_TEMPLATE_DIR_NAME}/${base_loss_function_name}-neigh0/model.h5"
set output_dir_name="${TOP_OUTPUT_DIR_NAME}/${base_loss_function_name}_wavelets${wavelet_transform_targets}_min-resolution-deg=${min_target_resolution_deg}_max-resolution-deg=${max_target_resolution_deg}"
echo $output_dir_name

python3 -u "${CODE_DIR_NAME}/train_neural_net.py" \
--training_predictor_dir_name="${TRAINING_PREDICTOR_DIR_NAME}" \
--validn_predictor_dir_name="${VALIDN_PREDICTOR_DIR_NAME}" \
--training_target_dir_name="${TRAINING_TARGET_DIR_NAME}" \
--validn_target_dir_name="${VALIDN_TARGET_DIR_NAME}" \
--input_model_file_name="${template_file_name}" \
--output_model_dir_name="${output_dir_name}" \
--lead_time_seconds=3600 \
--lag_times_seconds 0 1200 2400 \
--include_time_dimension=0 \
--omit_north_radar=1 \
--normalize=1 \
--uniformize=1 \
--fourier_transform_targets=${fourier_transform_targets} \
--wavelet_transform_targets=${wavelet_transform_targets} \
--min_target_resolution_deg=${min_target_resolution_deg} \
--max_target_resolution_deg=${max_target_resolution_deg} \
--num_examples_per_batch=${BATCH_SIZE} \
--max_examples_per_day_in_batch=`expr $BATCH_SIZE / 8` \
--use_partial_grids=1 \
--num_epochs=1000 \
--num_training_batches_per_epoch=64 \
--num_validn_batches_per_epoch=32
