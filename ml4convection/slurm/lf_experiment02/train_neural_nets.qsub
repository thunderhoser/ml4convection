#!/bin/tcsh

#SBATCH --job-name="train_neural_nets"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="batch"
#SBATCH --nodes=1
#SBATCH --ntasks=8           # 8 tasks per node
#SBATCH --cpus-per-task=2
#SBATCH --ntasks-per-node=8  # 8 GPUs per node
#SBATCH --exclusive
#SBATCH --time=30:00:00
#SBATCH --array=1-48
#SBATCH --exclude=h28n12
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=train_neural_nets_%A_%a.out

module load cuda/10.1
source /scratch2/BMC/gsd-hpcs/Jebb.Q.Stewart/conda3.7/etc/profile.d/conda.csh
conda activate base

set CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_standalone/ml4convection"
set TOP_TEMPLATE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_models/lf_experiment02/templates"
set TOP_OUTPUT_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_models/lf_experiment02"

set BATCH_SIZE=60

set TRAINING_PREDICTOR_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_project/predictors/quality_controlled/partial_grids"
set VALIDN_PREDICTOR_DIR_NAME="${TRAINING_PREDICTOR_DIR_NAME}"
set TRAINING_TARGET_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml4convection_project/targets/new_echo_classification/no_tracking/omit_north_radar/partial_grids"
set VALIDN_TARGET_DIR_NAME="${TRAINING_TARGET_DIR_NAME}"

set LOSS_FUNCTION_NAMES=("brier-0.0000d-0.0125d" "brier-0.0125d-0.0250d" "brier-0.0250d-0.0500d" "brier-0.0500d-0.1000d" "brier-0.1000d-0.2000d" "brier-0.2000d-0.4000d" "brier-0.4000d-0.8000d" "brier-0.8000d-infd" "fss-0.0000d-0.0125d" "fss-0.0125d-0.0250d" "fss-0.0250d-0.0500d" "fss-0.0500d-0.1000d" "fss-0.1000d-0.2000d" "fss-0.2000d-0.4000d" "fss-0.4000d-0.8000d" "fss-0.8000d-infd" "csi-0.0000d-0.0125d" "csi-0.0125d-0.0250d" "csi-0.0250d-0.0500d" "csi-0.0500d-0.1000d" "csi-0.1000d-0.2000d" "csi-0.2000d-0.4000d" "csi-0.4000d-0.8000d" "csi-0.8000d-infd" "all-class-iou-neigh0" "all-class-iou-neigh1" "all-class-iou-neigh2" "all-class-iou-neigh3" "all-class-iou-neigh4" "all-class-iou-neigh6" "all-class-iou-neigh8" "all-class-iou-neigh12" "all-class-iou-0.0000d-0.0125d" "all-class-iou-0.0125d-0.0250d" "all-class-iou-0.0250d-0.0500d" "all-class-iou-0.0500d-0.1000d" "all-class-iou-0.1000d-0.2000d" "all-class-iou-0.2000d-0.4000d" "all-class-iou-0.4000d-0.8000d" "all-class-iou-0.8000d-infd" "dice-0.0000d-0.0125d" "dice-0.0125d-0.0250d" "dice-0.0250d-0.0500d" "dice-0.0500d-0.1000d" "dice-0.1000d-0.2000d" "dice-0.2000d-0.4000d" "dice-0.4000d-0.8000d" "dice-0.8000d-infd")
set loss_function_name=${LOSS_FUNCTION_NAMES[$SLURM_ARRAY_TASK_ID]}

set template_file_name="${TOP_TEMPLATE_DIR_NAME}/${loss_function_name}/model.h5"
set output_dir_name="${TOP_OUTPUT_DIR_NAME}/${loss_function_name}"
echo $output_dir_name

python3 -u "${CODE_DIR_NAME}/train_neural_net.py" \
--training_predictor_dir_name="${TRAINING_PREDICTOR_DIR_NAME}" \
--validn_predictor_dir_name="${VALIDN_PREDICTOR_DIR_NAME}" \
--training_target_dir_name="${TRAINING_TARGET_DIR_NAME}" \
--validn_target_dir_name="${VALIDN_TARGET_DIR_NAME}" \
--input_model_file_name="${template_file_name}" \
--output_model_dir_name="${output_dir_name}" \
--lead_time_seconds=3600 \
--lag_times_seconds 0 1200 2400 \
--include_time_dimension=0 \
--omit_north_radar=1 \
--normalize=1 \
--uniformize=1 \
--num_examples_per_batch=${BATCH_SIZE} \
--max_examples_per_day_in_batch=`expr $BATCH_SIZE / 8` \
--use_partial_grids=1 \
--num_epochs=1000 \
--num_training_batches_per_epoch=64 \
--num_validn_batches_per_epoch=32
